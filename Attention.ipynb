{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.data.loader import load_multi30k, load_WMT14\n",
    "from src.data.raw_to_proc import proc_WMT14, proc_multi30k, create_WMT14_samp\n",
    "from src.models.lstm_rnn import SimpleEncoder,SimpleEncoderVLS,SimpleDecoder\n",
    "from src.models import fit,translate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_iter, valid_iter, SRC, TRG = load_multi30k(1)\n",
    "src, trg = 'de','en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_words_src = len(SRC.vocab)\n",
    "n_words_trg = len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pad_src_id = SRC.vocab.stoi['<pad>']\n",
    "pad_trg_id = TRG.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trg_sos_id = TRG.vocab.stoi['<SOS>']\n",
    "trg_eos_id = TRG.vocab.stoi['<EOS>']\n",
    "src_eos_id = SRC.vocab.stoi['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_len = len(train_iter)\n",
    "val_len = len(valid_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Setting some basic parameters for network - sizes of embedding vectors and hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedding_size_src = 300\n",
    "embedding_size_trg = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_hidden_src = 200\n",
    "n_hidden_trg = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the most basic type of model we will use. For practical reasons, we are going to treat encoder and decoder separately. The pair below is only capable of single-sequence input processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "enc = SimpleEncoder(n_words_src,embedding_size_src,n_hidden_src).cuda()\n",
    "dec = SimpleDecoder(n_words_trg,embedding_size_trg,n_hidden_trg).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The pair below is able to work with batch of VLS (variable length sequences). Because we are working with RNNs, parallel computation isn't as straightforward as in the case of other architectures, especially in our case with big amount of short sequences of variable lengths. The real dealbreaker here is the fact that we have pairs of sequences, meaning we can't simply join together the ones with least padding. As a solution, I have tried to implement PyTorch PackedSequence objects, with at least some succes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "enc = SimpleEncoderVLS(n_words_src,embedding_size_src,n_hidden_src,pad_src_id).cuda()\n",
    "dec = SimpleDecoder(n_words_trg,embedding_size_trg,n_hidden_trg).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The hyperparameters used can be changed as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2,3],[4,5,6]]); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 6.],\n",
       "        [1., 2., 3.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonoGRU(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.state = nn.ModuleList([nn.Linear(n_input,n_hidden),nn.Linear(n_hidden,n_hidden)])\n",
    "        self.reset = nn.ModuleList([nn.Linear(n_input,n_hidden),nn.Linear(n_hidden,n_hidden)])\n",
    "        self.update = nn.ModuleList([nn.Linear(n_input,n_hidden),nn.Linear(n_hidden,n_hidden)])\n",
    "        \n",
    "    def forward(self,*args):\n",
    "        inps = args[0]\n",
    "        bs = inps.size(1)\n",
    "        if len(args) == 1:\n",
    "            hidd = self.init_hidden(bs)\n",
    "        elif len(args) == 2:\n",
    "            hidd = args[1]\n",
    "            \n",
    "        hidds = torch.zeros(inps.size(0),*hidd.size())\n",
    "        for i,inp in enumerate(inps):\n",
    "            r = torch.sigmoid(self.reset[0](inp)+self.reset[1](hidd))\n",
    "            z = torch.sigmoid(self.update[0](inp)+self.update[1](hidd))\n",
    "            new_s = torch.tanh(self.state[0](inp)+self.state[1](r*hidd))\n",
    "            hidd = z*hidd + (1-z)*new_s\n",
    "            hidds[i] = hidd\n",
    "        \n",
    "        return hidds\n",
    "    \n",
    "    def init_hidden(self,bs):\n",
    "        return torch.zeros(bs,self.n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.gru = MonoGRU(n_input, n_hidden)\n",
    "        \n",
    "    def forward(self,*args):\n",
    "        # f = forward, b = backward\n",
    "        inps_f = args[0]\n",
    "        inps_b = inps_f.flip(0)\n",
    "        bs = inps_f.size(1)\n",
    "        \n",
    "        if len(args) == 1:\n",
    "            hidd = self.init_hidden(bs)\n",
    "        elif len(args) == 2:\n",
    "            hidd = args[1]\n",
    "            \n",
    "        hidd_f = hidd[0]\n",
    "        hidd_b = hidd[1]\n",
    "            \n",
    "        new_hidd_f = self.gru(inps_f,hidd_f)\n",
    "        new_hidd_b = self.gru(inps_b,hidd_b)\n",
    "            \n",
    "        return torch.cat((new_hidd_f,new_hidd_b),2)\n",
    "            \n",
    "    def init_hidden(self,bs):\n",
    "        return torch.zeros(2,bs,self.n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    '''\n",
    "    - All dimensions should be the same as for torch implementation of GRU\n",
    "    - Currently this implementation is hard-coded to one layer\n",
    "    - Because of 1 layer, hidden = out[-1] (GRU only outputs out)\n",
    "    '''\n",
    "    def __init__(self,n_input, n_hidden, bidirectional = False):\n",
    "        super().__init__()\n",
    "        if not bidirectional:\n",
    "            self.gru = MonoGRU(n_input, n_hidden)\n",
    "        else:\n",
    "            self.gru = BiGRU(n_input, n_hidden)\n",
    "            \n",
    "    def forward(self,*args):\n",
    "        return self.gru(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = GRU(5,5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.Tensor([[5,4,3,2,1],[1,2,3,4,5]])[:,None,:]; t1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 10])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru(t1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnGRU(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnEncoder(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Setting up some training parameters - optimizers, learning rate, loss function, number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "opt_enc = optim.SGD(enc.parameters(),5e-2)\n",
    "opt_dec = optim.SGD(dec.parameters(),5e-2)\n",
    "loss_fn = F.nll_loss\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Depending on your choice of data and models you should choose one of those. In order to manipulate length of training you can change the end_train and end_val parameters. I suggest using print_every size 5 times smaller than end_train. It's also good idea to set teacher_forcing to zero in the later stages of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "Train: 5.350942218092066 \n",
      "Valid: 5.33647784655381 \n",
      "\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "Train: 5.316898484907207 \n",
      "Valid: 5.309218650738869 \n",
      "\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "Train: 5.200537258116216 \n",
      "Valid: 5.200820240043324 \n",
      "\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "Train: 4.903539925049513 \n",
      "Valid: 4.922513263465385 \n",
      "\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "100.0% done\n",
      "Train: 4.850779306371301 \n",
      "Valid: 4.83715390653535 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit.Multi30k(enc,dec,train_iter,valid_iter,epochs,opt_enc,opt_dec,loss_fn,n_words_trg, trg_sos_id,\n",
    "             end_train=train_len,end_val=val_len,print_every=int(train_len/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6% done\n",
      "1.1% done\n",
      "1.7% done\n",
      "2.2% done\n",
      "2.8% done\n",
      "3.3% done\n",
      "3.9% done\n",
      "4.4% done\n",
      "5.0% done\n",
      "5.5% done\n",
      "6.1% done\n",
      "6.6% done\n",
      "7.2% done\n",
      "7.7% done\n",
      "8.3% done\n",
      "8.8% done\n",
      "9.4% done\n",
      "9.9% done\n",
      "10.5% done\n",
      "11.0% done\n",
      "11.6% done\n",
      "12.2% done\n",
      "12.7% done\n",
      "13.3% done\n",
      "13.8% done\n",
      "14.4% done\n",
      "14.9% done\n",
      "15.5% done\n",
      "16.0% done\n",
      "16.6% done\n",
      "17.1% done\n",
      "17.7% done\n",
      "18.2% done\n",
      "18.8% done\n",
      "19.3% done\n",
      "19.9% done\n",
      "Train: 5.073108215524693 \n",
      "Valid: 5.1813329735187565 \n",
      "\n",
      "20.4% done\n",
      "21.0% done\n",
      "21.5% done\n",
      "22.1% done\n",
      "22.7% done\n",
      "23.2% done\n",
      "23.8% done\n",
      "24.3% done\n",
      "24.9% done\n",
      "25.4% done\n",
      "26.0% done\n",
      "26.5% done\n",
      "27.1% done\n",
      "27.6% done\n",
      "28.2% done\n",
      "28.7% done\n",
      "29.3% done\n",
      "29.8% done\n",
      "30.4% done\n",
      "30.9% done\n",
      "31.5% done\n",
      "32.0% done\n",
      "32.6% done\n",
      "33.1% done\n",
      "33.7% done\n",
      "34.3% done\n",
      "34.8% done\n",
      "35.4% done\n",
      "35.9% done\n",
      "36.5% done\n",
      "37.0% done\n",
      "37.6% done\n",
      "38.1% done\n",
      "38.7% done\n",
      "39.2% done\n",
      "39.8% done\n",
      "Train: 5.2609340686990755 \n",
      "Valid: 5.150072846749817 \n",
      "\n",
      "40.3% done\n",
      "40.9% done\n",
      "41.4% done\n",
      "42.0% done\n",
      "42.5% done\n",
      "43.1% done\n",
      "43.6% done\n",
      "44.2% done\n",
      "44.8% done\n",
      "45.3% done\n",
      "45.9% done\n",
      "46.4% done\n",
      "47.0% done\n",
      "47.5% done\n",
      "48.1% done\n",
      "48.6% done\n",
      "49.2% done\n",
      "49.7% done\n",
      "50.3% done\n",
      "50.8% done\n",
      "51.4% done\n",
      "51.9% done\n",
      "52.5% done\n",
      "53.0% done\n",
      "53.6% done\n",
      "54.1% done\n",
      "54.7% done\n",
      "55.2% done\n",
      "55.8% done\n",
      "56.4% done\n",
      "56.9% done\n",
      "57.5% done\n",
      "58.0% done\n",
      "58.6% done\n",
      "59.1% done\n",
      "59.7% done\n",
      "Train: 5.161083303316675 \n",
      "Valid: 5.251803461951439 \n",
      "\n",
      "60.2% done\n",
      "60.8% done\n",
      "61.3% done\n",
      "61.9% done\n",
      "62.4% done\n",
      "63.0% done\n",
      "63.5% done\n",
      "64.1% done\n",
      "64.6% done\n",
      "65.2% done\n",
      "65.7% done\n",
      "66.3% done\n",
      "66.9% done\n",
      "67.4% done\n",
      "68.0% done\n",
      "68.5% done\n",
      "69.1% done\n",
      "69.6% done\n",
      "70.2% done\n",
      "70.7% done\n",
      "71.3% done\n",
      "71.8% done\n",
      "72.4% done\n",
      "72.9% done\n",
      "73.5% done\n",
      "74.0% done\n",
      "74.6% done\n",
      "75.1% done\n",
      "75.7% done\n",
      "76.2% done\n",
      "76.8% done\n",
      "77.3% done\n",
      "77.9% done\n",
      "78.5% done\n",
      "79.0% done\n",
      "79.6% done\n",
      "Train: 5.275578630090964 \n",
      "Valid: 5.228825560723893 \n",
      "\n",
      "80.1% done\n",
      "80.7% done\n",
      "81.2% done\n",
      "81.8% done\n",
      "82.3% done\n",
      "82.9% done\n",
      "83.4% done\n",
      "84.0% done\n",
      "84.5% done\n",
      "85.1% done\n",
      "85.6% done\n",
      "86.2% done\n",
      "86.7% done\n",
      "87.3% done\n",
      "87.8% done\n",
      "88.4% done\n",
      "89.0% done\n",
      "89.5% done\n",
      "90.1% done\n",
      "90.6% done\n",
      "91.2% done\n",
      "91.7% done\n",
      "92.3% done\n",
      "92.8% done\n",
      "93.4% done\n",
      "93.9% done\n",
      "94.5% done\n",
      "95.0% done\n",
      "95.6% done\n",
      "96.1% done\n",
      "96.7% done\n",
      "97.2% done\n",
      "97.8% done\n",
      "98.3% done\n",
      "98.9% done\n",
      "99.4% done\n",
      "Train: 5.17569672820544 \n",
      "Valid: 5.021304919262125 \n",
      "\n",
      "100.0% done\n"
     ]
    }
   ],
   "source": [
    "fit.Multi30k_VLS(enc,dec,train_iter,valid_iter,epochs,opt_enc,opt_dec,loss_fn,n_words_trg, trg_sos_id,\n",
    "                 pad_src_id, pad_trg_id,end_train=int(train_len/5),end_val=int(val_len/5),\n",
    "                 print_every=int(train_len/(5*5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "11.9% done\n",
      "12.9% done\n",
      "13.9% done\n",
      "14.9% done\n",
      "15.9% done\n",
      "16.9% done\n",
      "17.9% done\n",
      "18.9% done\n",
      "19.9% done\n",
      "Train: 7.183032462994258 \n",
      "Valid: 7.198314721385638 \n",
      "\n",
      "20.9% done\n",
      "21.9% done\n",
      "22.9% done\n",
      "23.9% done\n",
      "24.9% done\n",
      "25.9% done\n",
      "26.9% done\n",
      "27.9% done\n",
      "28.9% done\n",
      "29.9% done\n",
      "30.9% done\n",
      "31.9% done\n",
      "32.9% done\n",
      "33.9% done\n",
      "34.9% done\n",
      "35.8% done\n",
      "36.8% done\n",
      "37.8% done\n",
      "38.8% done\n",
      "39.8% done\n",
      "Train: 6.854988818367322 \n",
      "Valid: 6.9456145738561945 \n",
      "\n",
      "40.8% done\n",
      "41.8% done\n",
      "42.8% done\n",
      "43.8% done\n",
      "44.8% done\n",
      "45.8% done\n",
      "46.8% done\n",
      "47.8% done\n",
      "48.8% done\n",
      "49.8% done\n",
      "50.8% done\n",
      "51.8% done\n",
      "52.8% done\n",
      "53.8% done\n",
      "54.8% done\n",
      "55.8% done\n",
      "56.8% done\n",
      "57.8% done\n",
      "58.7% done\n",
      "59.7% done\n",
      "Train: 7.104311967889468 \n",
      "Valid: 6.87798385322094 \n",
      "\n",
      "60.7% done\n",
      "61.7% done\n",
      "62.7% done\n",
      "63.7% done\n",
      "64.7% done\n",
      "65.7% done\n",
      "66.7% done\n",
      "67.7% done\n",
      "68.7% done\n",
      "69.7% done\n",
      "70.7% done\n",
      "71.7% done\n",
      "72.7% done\n",
      "73.7% done\n",
      "74.7% done\n",
      "75.7% done\n",
      "76.7% done\n",
      "77.7% done\n",
      "78.7% done\n",
      "79.7% done\n",
      "Train: 6.870434527595838 \n",
      "Valid: 7.00452895462513 \n",
      "\n",
      "80.7% done\n",
      "81.7% done\n",
      "82.6% done\n",
      "83.6% done\n",
      "84.6% done\n",
      "85.6% done\n",
      "86.6% done\n",
      "87.6% done\n",
      "88.6% done\n",
      "89.6% done\n",
      "90.6% done\n",
      "91.6% done\n",
      "92.6% done\n",
      "93.6% done\n",
      "94.6% done\n",
      "95.6% done\n",
      "96.6% done\n",
      "97.6% done\n",
      "98.6% done\n",
      "99.6% done\n",
      "Train: 7.0077323615550995 \n",
      "Valid: 6.898369203011195 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit.WMT14(enc,dec,train_iter,valid_iter,epochs,opt_enc,opt_dec,loss_fn,n_words_trg,trg_sos_id,\n",
    "          end=int(train_len/10),print_every=int(train_len/(10*5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fit functions of Multi30k automatically save progress after each epoch, but these functions can be used to manually save / load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_path_Multi30k = 'models/LSTM_RNN/Multi30k/'\n",
    "model_path_WMT14 = 'models/LSTM_RNN/WMT14/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Loading / saving Multi30k models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "enc.load_state_dict(torch.load(f'{model_path_Multi30k}enc.pt'))\n",
    "dec.load_state_dict(torch.load(f'{model_path_Multi30k}dec.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), f'{model_path_Multi30k}enc.pt')\n",
    "torch.save(dec.state_dict(), f'{model_path_Multi30k}dec.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Loading / saving WMT14 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "enc.load_state_dict(torch.load(f'{model_path_WMT14}enc.pt'))\n",
    "dec.load_state_dict(torch.load(f'{model_path_WMT14}dec.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), f'{model_path_WMT14}enc.pt')\n",
    "torch.save(dec.state_dict(), f'{model_path_WMT14}dec.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We won't dig deep into the model's workings / performance. For now, we will only look at examples of the model's translations. This will give us some insight into it's capabilities and we will also be able to better interpret the results given by applying custom metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get new example, simply reload the line below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rand_ex = next(iter(valid_iter))\n",
    "ex_src,ex_trg = getattr(rand_ex,src),getattr(rand_ex,trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These two cells show what we are going to translate and it's human translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mehrere',\n",
       " 'menschen',\n",
       " 'stehen',\n",
       " 'in',\n",
       " 'der',\n",
       " 'dämmerung',\n",
       " 'in',\n",
       " 'der',\n",
       " 'nähe',\n",
       " 'einiger',\n",
       " 'bäume',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[SRC.vocab.itos[x] for x in ex_src[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS>',\n",
       " 'several',\n",
       " 'people',\n",
       " 'are',\n",
       " 'standing',\n",
       " 'near',\n",
       " 'trees',\n",
       " 'at',\n",
       " 'dusk',\n",
       " '<EOS>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TRG.vocab.itos[x] for x in ex_trg[:,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we use our model to translate the sentence. Choose appropriate translate function (Multi30k, Multi30k_VLS and WMT14 respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent_ids = translate.Multi30k(enc,dec,trg_sos_id,trg_eos_id,ex_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent_ids = translate.Multi30k_VLS(enc,dec,trg_sos_id,trg_eos_id,pad_src_id,ex_src[:,0][:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent_ids = translate.WMT14(enc,dec,trg_sos_id,trg_eos_id,ex_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can take a look at the sentence our model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['several', 'people', 'are', 'people', 'in', 'near', 'a', 'a', 'a', '<EOS>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TRG.vocab.itos[x] for x in sent_ids]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
